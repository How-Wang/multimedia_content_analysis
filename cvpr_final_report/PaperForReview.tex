% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)
\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version


\usepackage{CJKutf8} % For chinese output

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor}
\graphicspath{ {./images/} }

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}
\begin{CJK*}{UTF8}{bsmi}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Google - Isolated Sign Language Recognition}

\author{王浩 \\
國立成功大學　資訊工程學系\\
F74082141\\
{\tt\small Howard.H.Wang.23@gmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
杜孟聰\\
國立成功大學　資訊工程學系\\
F74082028\\
{\tt\small F74082028@gs.ncku.edu.tw}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
   This paper aims to classify isolated American Sign Language (ASL) signs using a deep learning model. 
   To achieve this, we trained the model on labeled landmark data extracted via the MediaPipe Holistic Solution. 
   Our approach leverages the accuracy and reliability of this technology to enhance the robustness of the model and improve its performance. 
   By accurately classifying ASL signs, our method has the potential to support individuals with hearing impairments and 
   enable better communication between hearing and non-hearing populations.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Deaf children born to hearing parents are at risk of Language Deprivation Syndrome if they don't have access to a fully developed language early in life. 
However, learning American Sign Language (ASL) can be time-consuming and resource-intensive for parents. 
Automatic sign language recognition technology can help overcome this barrier by recognizing and translating signs into text or speech.

In this paper, we propose a novel approach to isolated sign language recognition using computer vision techniques and machine learning algorithms. 
Our approach improves accuracy, robustness, and speed, as demonstrated on a publicly available dataset.

{\color{blue} this 
is the test for change 
color}

%-------------------------------------------------------------------------
\subsection{American Sign Language}

American Sign Language (ASL) is a language used primarily by members of the Deaf community in North America. 
It is not a visual representation of English, but has its own unique grammar, syntax, and vocabulary. 
ASL is a visual-gestural language, meaning that it uses facial expressions, body language, and hand movements to convey meaning.

\subsection{Isolated Sign Language Recognition}

ISLR stands for Isolated Sign Language Recognition, which is the task of recognizing individual signs or tokens called glosses from a given segment of signing video clip.
It is the process of recognizing sign language gestures performed by a person in isolation, without considering the context or the surrounding gestures. 
We will train a machine learning model that can accurately recognize isolated sign language signs and classify them into the correct sign category.

%------------------------------------------------------------------------
\section{Explorative Data Analysis}
\label{sec:formatting}

%-------------------------------------------------------------------------
\subsection{Raw Data Format}
MediaPipe Holistic Solution~\cite{https://doi.org/10.48550/arxiv.1906.08172} is a powerful, easy-to-use software tool that can detect and track multiple human body parts and gestures in real-time video streams. 
\includegraphics[width=80mm]{holistic_pipeline_example}
The way that MediaPipe Holistic Solution records facial expressions, body language, and hand movements is by using landmarks.

Landmarks or keypoints are like dots that are placed on important areas of an object or a person's body. 
These dots help a computer to understand where these important areas are and how they are moving.

%------------------------------------------------------------------------
\subsection{Data Selection}
{\color{blue} 
The data utilized in this study was obtained from Mediapipe’s Holistic solution and contains landmarks from three categories: Face, Pose (body), and Hand.
In accordance with established linguistic norms, movements below the chest are generally not utilized in sign language for conveying linguistic meanings, therefore, we will exclude landmarks from the hips down and retain only the relevant landmarks for Pose landmarks. Although facial expressions are not an integral part of signing, they can still play a role in identifying words that require the signer to point to parts of the face. Furthermore, facial expressions can provide subtle cues to differentiate between similar signs. Thus, we will utilize landmarks for the major features of the face: eyes, eyebrows, nose, and lips, and trim out the rest. Given the crucial role of hands in sign language, all Hand landmarks will be included in our training data. In summary, our model will be trained using landmarks from the arms, hands, and major facial features to recognize isolated signs.
}

%------------------------------------------------------------------------
\subsection{Data Imputation}
{\color{blue} 

As described in the competition’s guidelines, not all frames’ Hand data are complete. We have identified two categories with respect to the nature of the missed Hand data:
The first category is where the hand rests idle below the waist. In sign language, there are signs that require only one hand to convey meaning. In such cases, the signer usually rests the other hand at their side, and the missed landmarks on this hand can be disregarded.
The second category involves hands that are raised around the chest or face. A hand in these positions usually indicate that it is actively being used to sign. In such cases, certain techniques need to be employed to deal with these lost data. In cases where hand data is lost in some frames but not others, interpolation can be employed to fill the gaps. The reliability of this method may depend on the number of subtle movements between the frames and the size of the gaps. Alternatively, if the loss of hand data is too substantial (e.g., if no hand data exists across the entire clip), discarding the clip altogether may be the best option. Another viable approach to imputing missed data is to use statistical methods such as averaging measures from other data with the same label. However, complications may arise when the number of frames differs across clips, even when they correspond to the same sign.

}

%------------------------------------------------------------------------

\section{Input Data Format}
\label{sec:formatting}

%-------------------------------------------------------------------------

\subsection{Hierarchical Encoding}
{\color{blue} 
Rather than plotting all landmarks on a shared “global” coordinate system, we will map landmarks onto a hierarchy of local coordinate systems based on the articulation of joints and muscles. We propose that organizing landmark coordinates in a hierarchical structure may help reduce the impact of variations in different realizations of the same sign.
Hypothesis 1. Organizing landmark coordinates in a hierarchical structure in a gesture recognition task can help reduce the impact of variations in different realizations of the same sign.
The reasoning to Hypothesis 1. is that, for most signs, it does not matter whether the hands are slightly below the chest or above it nor does it matter whether it is slightly outside of shoulder width or slightly within, i.e. slight variations in the relative position does not typically result in significant semantic differences. Such variations may be observed across differnt signers with differnt body structures and even in the same signer across different instances and, with one global coordinate system, are reflected systematically on all landmark coordinates of the hands. If we instead encode the hand landmarks in the wrists coordinate system, that variation will be eliminated in the hands and only reflected in the wrists and elbows. Since the number of landmarks impacted by this variation significantly decreases in the hierarchical scheme, we assert that this will in general make the model more robust to variations that are not semantically significant.
}
\includegraphics[width=80mm]{hierarchical_scheme}

%------------------------------------------------------------------------

\subsection{Orientation-Focused Encoding}
{\color{blue} 
On top of mapping landmarks onto a hierarchy of coordinate systems, we aim to investigate the impact of an extra encoding scheme on the performance of the model. Specifically, we will explore the effect of encoding only the orientation of landmarks with respect to their origin (i.e., parent) on the final results. To achieve this, we will scale all landmarks down to a uniform size.
On one hand, this approach may help eliminate slight variations in landmark coordinates that arise from individual differences in body structure, such as differences in bone length. On the other hand, since only the x and y coordinates are used in this specific implementation, rotations that do not result in a change in orientation on the xy plane may not be reliably captured. In contrast, if the norm or length of the landmark vectors is also encoded, at least some information about the degree of rotation could be partially captured. For example, the shorter the forearm appears, the more it has rotated towards the observer.
}

%------------------------------------------------------------------------
\subsection{Temporal Normalization by Down-/Upsampling}
{\color{blue} 
Accounting for the different signing speeds among individuals is an important consideration in training a robust sign language recognition model. In practice, signers can vary greatly in their signing speed, and this can result in variations in the rate at which landmarks move from frame to frame. To ensure that our model is not overly sensitive to such differences, we will downsample or upsample the clips to achieve a uniform number of frames. By doing so, we can eliminate any potential errors introduced by variations in the speed at which landmarks move.
}
%------------------------------------------------------------------------

\subsection{Landmark Mirroring}
{\color{blue} 
In sign language, signers commonly rely on a dominant hand to perform the majority of signs, with the specific hand varying among individuals.~\cite{Sign} When signing words that require only one hand, signers typically have the freedom to choose which hand to use. For words that require both hands and involve asymmetric movements, instructions usually designate separate sets of movements for the dominant hand and for the non-dominant hand; it is ultimately up to the signer to decide which hand is dominant.

Thus, for most sign language gestures, flipping the gesture horizontally does not alter its meaning. We have thoroughly examined the dataset and identified no labels such as “left” or “right” for which the choice of dominant hand may affect the meaning. We have thus eliminated concerns for the impact of hand selection in this specific dataset; with that in mind, in our implementation, we will augment the dataset by creating a mirrored version of each clip, enabling the model to learn to recognize the gesture regardless of which hand is designated as dominant.
}
%=========here i am ~~~
%------------------------------------------------------------------------

\section{Model}
{\color{blue}
The Transformer architecture has demonstrated efficacy in sequence-based tasks for dynamic gesture recognition~\cite{TR}. It is important to note that our task differs from the task described in~\cite{TR}, as our input data is comprised of landmark coordinates rather than bitmap images. Specifically, the authors in~\cite{TR} utilize a Convolutional Neural Network (CNN) architecture, ResNet-18, to convert bitmap images into embeddings before feeding them into the Transformer. Despite this difference, we find the Transformer architecture to be a suitable model for our task given its success in a similar domain. Our proposed implementation will use the Transformer with sequences of landmark coordinates as input, and the model will output probabilities for each label.

Our proposed model will consist of a Transformer encoder, followed by a multi-layer perceptron (MLP) decoder. The encoder will be responsible for encoding the input sequence of landmark coordinates into a fixed-length representation, which will then be passed to the decoder to produce the final output probabilities for each label. The MLP decoder will consist of two fully connected layers with ReLU activation functions and a final softmax layer to produce the output probabilities.

To improve the model’s ability to capture temporal dependencies between landmarks, we will also experiment with the use of a Recurrent Neural Network (RNN) architecture as an alternative to the Transformer encoder. In particular, we will explore the use of a Long Short-Term Memory (LSTM) network, which has been shown to be effective in capturing long-term dependencies in sequential data.
}

%------------------------------------------------------------------------
\section{Expected Result}
Our proposed state-of-the-art model, which utilizes the input data of every video frame's landmarks captured by the MediaPipe Holistic Solution, 
is expected to significantly improve the accuracy and speed of isolated sign language recognition. 
We anticipate that the model will achieve high recognition rates for individual signs, 
thus aiding in the communication and language acquisition process of individuals who are deaf or hard of hearing. 
We believe that our research will contribute to the development of more sophisticated and efficient sign language recognition technology, 
making communication more accessible for the Deaf community


%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{CJK*}
\end{document}

